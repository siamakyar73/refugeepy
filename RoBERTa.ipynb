{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSM7wjoNJyg8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gyn-HX6ZJ6Qy"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"Bookf.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZmOeFECJ-ia"
      },
      "outputs": [],
      "source": [
        "def count_words(text):\n",
        "    words = re.findall(r'\\w+', text)\n",
        "    return len(words)\n",
        "df['word_count'] = df['text'].apply(count_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwm8CLJOKCFL"
      },
      "outputs": [],
      "source": [
        "punctuation = string.punctuation\n",
        "df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt-X-VX7KDcI"
      },
      "outputs": [],
      "source": [
        "df=df.drop('length', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkcrDPPlKFCK"
      },
      "outputs": [],
      "source": [
        "def get_text_lengths(text_column):\n",
        "    return text_column.apply(len)\n",
        "\n",
        "# Apply the function to the text column\n",
        "df['text_length'] = get_text_lengths(df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8h1713iKNfQ",
        "outputId": "1ef2e8d0-7a87-4452-9c10-2b8792c11306"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import textblob\n",
        "from textblob import Word\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return \" \".join(filtered_words)\n",
        "def without_leading_trailing_whitespace(text):\n",
        "  return text.strip()\n",
        "def lowercase(text):\n",
        "  return text.lower()\n",
        "def remove_special_characters(text):\n",
        "  pat = r'[^a-zA-z0-9]'\n",
        "  return re.sub(pat, ' ', text)\n",
        "def special_text(text):\n",
        "  sentences = re.split(r'\\.\\s', text)\n",
        "  sentences = [sentence for sentence in sentences if not sentence.startswith(\">\")]\n",
        "  cleaned_text = \". \".join(sentences)\n",
        "  return cleaned_text\n",
        "def no_http_links(text):\n",
        "  link_regex = r'http\\S+'\n",
        "  text = re.sub(link_regex, \" \", text)\n",
        "  return text\n",
        "def no_multi_punctuation(text):\n",
        "  pattern = r\"\\!+\"\n",
        "  text = re.sub(pattern, \"!\", text)\n",
        "  pattern = r\"\\?+\"\n",
        "  text = re.sub(pattern, \"?\", text)\n",
        "  pattern = r\"\\.+\"\n",
        "  text = re.sub(pattern, \".\", text)\n",
        "  return text\n",
        "def no_hash(text):\n",
        "  return re.sub(r'[\\#+]', \" \", text)\n",
        "def no_number(text):\n",
        "  text = re.sub('([0-9]+)', '', str(text))\n",
        "  return text\n",
        "def lem(text):\n",
        "  lemwords=[]\n",
        "  for word in text.split():\n",
        "    word=Word(word).lemmatize()\n",
        "    lemwords.append(word)\n",
        "  return \" \".join(lemwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lC6dNAJKQC4"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "  text= no_hash(text)\n",
        "  text= no_http_links(text)\n",
        "  text= without_leading_trailing_whitespace(text)\n",
        "  text= lowercase(text)\n",
        "  text= no_multi_punctuation(text)\n",
        "  text= remove_special_characters(text)\n",
        "  text= no_number(text)\n",
        "  text= remove_stopwords(text)\n",
        "  text= lem(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3p2yT4mKR8N"
      },
      "outputs": [],
      "source": [
        "df['cleaned_text']=df['text'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEyusiMiKTY0",
        "outputId": "232c6ea5-99ed-45a8-bcc9-a48a0ee7c624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpCtheYNJtO5",
        "outputId": "bc4f73ca-d969-46ad-ba27-914c113a5ca7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Initialize the RoBERTa model and tokenizer\n",
        "model_name = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Prepare your data\n",
        "X = df['cleaned_text'].tolist()\n",
        "y = df['supportive'].tolist()\n",
        "\n",
        "encoded_data = tokenizer(X, padding=True, truncation=True, return_tensors='pt')\n",
        "attention_masks = encoded_data['attention_mask']\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp, train_mask, temp_mask = train_test_split(\n",
        "    encoded_data['input_ids'], y, attention_masks, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test, val_mask, test_mask = train_test_split(\n",
        "    X_temp, y_temp, temp_mask, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create datasets and data loaders for train, validation, and test sets\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, train_mask, torch.tensor(y_train))\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, val_mask, torch.tensor(y_val))\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, test_mask, torch.tensor(y_test))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Define optimizer and loss criterion\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "epochs = 5\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False)\n",
        "    for inputs, mask, labels in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, attention_mask=mask)[0]\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        progress_bar.set_postfix({'Loss': loss.item()})\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXa9ns4CKfvB",
        "outputId": "9117aec4-2a60-4525-b6f6-db736a4d8768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Metrics:\n",
            "Validation Accuracy: 0.82\n",
            "Validation Precision: 0.61\n",
            "Validation Recall: 0.52\n",
            "Validation F1 Score: 0.56\n",
            "Confusion Matrix (Validation):\n",
            "[[606  64]\n",
            " [ 95 101]]\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on the validation set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_preds = []\n",
        "    for inputs, mask, labels in val_loader:\n",
        "        outputs = model(inputs, attention_mask=mask)[0]\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(y_val, all_preds)\n",
        "    val_precision = precision_score(y_val, all_preds)\n",
        "    val_recall = recall_score(y_val, all_preds)\n",
        "    val_f1 = f1_score(y_val, all_preds)\n",
        "    val_confusion = confusion_matrix(y_val, all_preds)\n",
        "\n",
        "    print('Validation Metrics:')\n",
        "    print(f'Validation Accuracy: {val_accuracy:.2f}')\n",
        "    print(f'Validation Precision: {val_precision:.2f}')\n",
        "    print(f'Validation Recall: {val_recall:.2f}')\n",
        "    print(f'Validation F1 Score: {val_f1:.2f}')\n",
        "    print('Confusion Matrix (Validation):')\n",
        "    print(val_confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FporQXJKhYU",
        "outputId": "e2d4a163-aa8d-4c77-963c-d9e20170a5e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Metrics:\n",
            "Test Accuracy: 0.81\n",
            "Test Precision: 0.60\n",
            "Test Recall: 0.45\n",
            "Test F1 Score: 0.51\n",
            "Confusion Matrix (Test):\n",
            "[[612  59]\n",
            " [108  88]]\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_preds = []\n",
        "    for inputs, mask, labels in test_loader:\n",
        "        outputs = model(inputs, attention_mask=mask)[0]\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.tolist())\n",
        "\n",
        "    test_accuracy = accuracy_score(y_test, all_preds)\n",
        "    test_precision = precision_score(y_test, all_preds)\n",
        "    test_recall = recall_score(y_test, all_preds)\n",
        "    test_f1 = f1_score(y_test, all_preds)\n",
        "    test_confusion = confusion_matrix(y_test, all_preds)\n",
        "\n",
        "    print('Test Metrics:')\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}')\n",
        "    print(f'Test Precision: {test_precision:.2f}')\n",
        "    print(f'Test Recall: {test_recall:.2f}')\n",
        "    print(f'Test F1 Score: {test_f1:.2f}')\n",
        "    print('Confusion Matrix (Test):')\n",
        "    print(test_confusion)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
