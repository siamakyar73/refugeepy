{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_SVthMQ3hRY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGLDNB073zAM"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"Bookf.csv\") # Bookf is the mixed dataset resulting from YouTube and Reddit datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLZY0pT831M0"
      },
      "outputs": [],
      "source": [
        "def count_words(text):\n",
        "    words = re.findall(r'\\w+', text)\n",
        "    return len(words)\n",
        "df['word_count'] = df['text'].apply(count_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpL9wYMq359k"
      },
      "outputs": [],
      "source": [
        "punctuation = string.punctuation\n",
        "df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6GgCy3Z38Ge"
      },
      "outputs": [],
      "source": [
        "df=df.drop('length', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcXkvwKq37nK"
      },
      "outputs": [],
      "source": [
        "def get_text_lengths(text_column):\n",
        "    return text_column.apply(len)\n",
        "\n",
        "df['text_length'] = get_text_lengths(df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8pm6qSO4LIa",
        "outputId": "cbb7f88f-5fe4-4368-bded-4c5e35d5e1f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import textblob\n",
        "from textblob import Word\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return \" \".join(filtered_words)\n",
        "def without_leading_trailing_whitespace(text):\n",
        "  return text.strip()\n",
        "def lowercase(text):\n",
        "  return text.lower()\n",
        "def remove_special_characters(text):\n",
        "  pat = r'[^a-zA-z0-9]'\n",
        "  return re.sub(pat, ' ', text)\n",
        "def special_text(text):\n",
        "  sentences = re.split(r'\\.\\s', text)\n",
        "  sentences = [sentence for sentence in sentences if not sentence.startswith(\">\")]\n",
        "  cleaned_text = \". \".join(sentences)\n",
        "  return cleaned_text\n",
        "def no_http_links(text):\n",
        "  link_regex = r'http\\S+'\n",
        "  text = re.sub(link_regex, \" \", text)\n",
        "  return text\n",
        "def no_multi_punctuation(text):\n",
        "  pattern = r\"\\!+\"\n",
        "  text = re.sub(pattern, \"!\", text)\n",
        "  pattern = r\"\\?+\"\n",
        "  text = re.sub(pattern, \"?\", text)\n",
        "  pattern = r\"\\.+\"\n",
        "  text = re.sub(pattern, \".\", text)\n",
        "  return text\n",
        "def no_hash(text):\n",
        "  return re.sub(r'[\\#+]', \" \", text)\n",
        "def no_number(text):\n",
        "  text = re.sub('([0-9]+)', '', str(text))\n",
        "  return text\n",
        "def lem(text):\n",
        "  lemwords=[]\n",
        "  for word in text.split():\n",
        "    word=Word(word).lemmatize()\n",
        "    lemwords.append(word)\n",
        "  return \" \".join(lemwords)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R11ab-Xp4Oc9"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "  text= no_hash(text)\n",
        "  text= no_http_links(text)\n",
        "  text= without_leading_trailing_whitespace(text)\n",
        "  text= lowercase(text)\n",
        "  text= no_multi_punctuation(text)\n",
        "  text= remove_special_characters(text)\n",
        "  text= no_number(text)\n",
        "  text= remove_stopwords(text)\n",
        "  text= lem(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iol9ssp4Qbk"
      },
      "outputs": [],
      "source": [
        "df['cleaned_text']=df['text'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqg3kTPUPLtL",
        "outputId": "bb69a4ea-5b8c-4923-89d1-b3c3355af5d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Set - Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       670\n",
            "           1       0.63      0.34      0.44       196\n",
            "\n",
            "    accuracy                           0.81       866\n",
            "   macro avg       0.73      0.64      0.66       866\n",
            "weighted avg       0.79      0.81      0.78       866\n",
            "\n",
            "\n",
            "Validation Set - Confusion Matrix:\n",
            "[[631  39]\n",
            " [129  67]]\n",
            "\n",
            "Test Set - Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       671\n",
            "           1       0.59      0.30      0.39       196\n",
            "\n",
            "    accuracy                           0.79       867\n",
            "   macro avg       0.71      0.62      0.64       867\n",
            "weighted avg       0.77      0.79      0.77       867\n",
            "\n",
            "\n",
            "Test Set - Confusion Matrix:\n",
            "[[631  40]\n",
            " [138  58]]\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = df['cleaned_text']\n",
        "y = df['supportive']\n",
        "\n",
        "# Split data into training (70%), validation (15%), and testing (15%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert text data to numerical features using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', C=1.0)\n",
        "\n",
        "# Fit the model on the training set\n",
        "svm_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = svm_classifier.predict(X_val_tfidf)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_report = classification_report(y_val, y_val_pred)\n",
        "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "print(\"Validation Set - Classification Report:\")\n",
        "print(val_report)\n",
        "\n",
        "print(\"\\nValidation Set - Confusion Matrix:\")\n",
        "print(val_conf_matrix)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = svm_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_report = classification_report(y_test, y_test_pred)\n",
        "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\nTest Set - Classification Report:\")\n",
        "print(test_report)\n",
        "\n",
        "print(\"\\nTest Set - Confusion Matrix:\")\n",
        "print(test_conf_matrix)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
